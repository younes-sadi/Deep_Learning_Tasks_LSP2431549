{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/younes-sadi/semantic-search-task2-LSP2431549/blob/main/Belkacem_Sadi_Task2_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8pJT7qKEIc6"
      },
      "source": [
        "Belkacem Sadi LSP_Id: 2431549 Task_2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project uses Retrieval-Augmented Generation (RAG) by embedding 5,000 real news articles using a transformer (MiniLM), storing the vectors in FAISS, retrieving top-k relevant entries for a user query, and generating a final answer using T5. The pipeline mimics the internal architecture of models like facebook/rag-sequence-nq while remaining lightweight and Colab-friendly"
      ],
      "metadata": {
        "id": "Y2zlg5AgHpw6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN2zffzjyH5v"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets==2.16.0\n",
        "!pip install -q transformers sentence-transformers faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7d9wtIFySdR"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets==2.16.0\n",
        "!pip install -q transformers sentence-transformers faiss-cpu\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ag_news\", split=\"train[:5000]\")\n",
        "texts = [x[\"text\"] for x in dataset]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAvfsDP860-G"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"text\": texts})\n",
        "df.to_csv(\"ag_news_5000.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhfEFN7opPSt"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = encoder.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y_2XHkZ4Avx"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "#  small generator model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "generator = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "query = \"What are the latest innovations in AI?\"\n",
        "query_embedding = encoder.encode([query], convert_to_numpy=True)\n",
        "D, I = index.search(query_embedding, k=3)\n",
        "\n",
        "# Combine top 3 results into a context\n",
        "context = \" \".join(texts[i] for i in I[0])\n",
        "prompt = f\"question: {query} context: {context}\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "outputs = generator.generate(**inputs)\n",
        "print(\"=== Answer ===\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcpeyJlLySNK"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "#  t5-large\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
        "generator = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
        "\n",
        "query = \"What are the latest news of sport \"\n",
        "\n",
        "query_embedding = encoder.encode([query], convert_to_numpy=True)\n",
        "\n",
        "D, I = index.search(query_embedding, k=5)\n",
        "\n",
        "# top 5\n",
        "context = \" \".join(texts[i] for i in I[0])\n",
        "\n",
        "prompt = f\"question: {query} context: {context}\"\n",
        "\n",
        "# 7. Tokenize the prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "outputs = generator.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    max_length=300,\n",
        "    min_length=50,\n",
        "    length_penalty=1.2,\n",
        "    num_beams=4,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# 9. Decode\n",
        "print(\"=== Answer ===\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANanM2tDDA_3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/younes-sadi/semantic-search-task2-LSP2431549.git\n",
        "!cd semantic-search-task2-LSP2431549\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a-vVRj5o_Fm"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnoPrfAmINwO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGFZUHY3uxbe"
      },
      "outputs": [],
      "source": [
        "%cd /content/semantic-search-task2-LSP2431549\n",
        "!git add Belkacem_Sadi_Task2_DL.ipynb\n",
        "!git commit -m \"Upload task2\"\n",
        "!git push\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4ZyVXCFRQl0KM5ilVyaVj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}